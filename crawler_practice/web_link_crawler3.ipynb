{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current url: https://afuntw.github.io/demo-crawling/demo-page/ex3/index1.html\n",
      "h2 tag: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.']\n",
      "\n",
      "Current url: https://afuntw.github.io/demo-crawling/demo-page/ex3/index2.html\n",
      "h2 tag: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.', 'About Heading 1', 'About Heading 2', 'About Heading 3']\n",
      "\n",
      "Current url: https://afuntw.github.io/demo-crawling/demo-page/ex3/index3.html\n",
      "h2 tag: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.', 'About Heading 1', 'About Heading 2', 'About Heading 3', 'Contact Heading 1', 'Contact Heading 2', 'Contact Heading 3', 'Reminds. If you have any problems or suggestions, please contact me, thank you.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wait_url = [\"https://afuntw.github.io/demo-crawling/demo-page/ex3/index1.html\"]\n",
    "viewed = []\n",
    "h2_tag =[]\n",
    "\n",
    "while wait_url!=[]:\n",
    "    \n",
    "    # 取出 wait list 裏面的第一個網址\n",
    "    url = wait_url.pop(0)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,\"lxml\")\n",
    "    \n",
    "    # 將當前頁面存入已經看過的清單\n",
    "    viewed.append(url)\n",
    "    \n",
    "    for i in soup.find_all(\"h2\"):\n",
    "        h2_tag.append(i.text)\n",
    "        \n",
    "    # 取得頁面中的 a tag\n",
    "    for i in soup.find_all(\"a\",href=True):\n",
    "        new_url = urljoin(url,i[\"href\"])\n",
    "        \n",
    "        # 過濾錨點, 不需要再對相同的網頁送 request\n",
    "        check1 = not re.match(\"#.*\",i[\"href\"])\n",
    "        \n",
    "        # 過濾協定, 只取 http 或是 https\n",
    "        # Hint: 若原本 href 是相對路徑則沒有協定, 要先透過 urljoin 取得絕對路徑\n",
    "        check2 = urlparse(new_url).scheme in [\"https\",\"http\"]\n",
    "        #print(urlparse(new_url))\n",
    "        \n",
    "        \n",
    "        # 過濾程式碼\n",
    "        check3 = not re.match(\"^javascript.*\",i[\"href\"])\n",
    "        \n",
    "        if check1 and check2 and check3:\n",
    "            if new_url not in wait_url and new_url not in viewed: \n",
    "                wait_url.append(new_url)\n",
    "                \n",
    "    print(\"Current url: {}\".format(url))\n",
    "    print(\"h2 tag: {}\".format(h2_tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
